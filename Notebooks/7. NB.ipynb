{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Test and Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data_path = '/workspaces/Final-Year-Project/Cleaned Data/TrainTestData.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop('Diabetes Status', axis=1)\n",
    "y = df['Diabetes Status']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š NB - Final dataset shape after scaling: (3268, 13)\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Display the shape after scaling\n",
    "print(f\"ðŸ“Š NB - Final dataset shape after scaling: {X_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes Classifier setup\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]  # Vary smoothing for more stability\n",
    "}\n",
    "\n",
    "# KFold cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# GridSearchCV setup with accuracy as the scoring parameter\n",
    "grid_search = GridSearchCV(nb, param_grid, scoring='accuracy', cv=kf, n_jobs=-1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Naive Bayes: {'var_smoothing': 1e-09}\n",
      "Best Accuracy with Best Parameters: 0.7255274409804665\n"
     ]
    }
   ],
   "source": [
    "# Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Best model from GridSearchCV\n",
    "best_nb = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best Parameters for Naive Bayes:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy with Best Parameters:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ NB Classification Report on Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70      1638\n",
      "           1       0.69      0.81      0.75      1630\n",
      "\n",
      "    accuracy                           0.73      3268\n",
      "   macro avg       0.73      0.73      0.73      3268\n",
      "weighted avg       0.74      0.73      0.73      3268\n",
      "\n",
      "ðŸŸ¦ NB - Confusion Matrix on Training Data:\n",
      "[[1056  582]\n",
      " [ 306 1324]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training data\n",
    "y_pred = best_nb.predict(X_scaled)\n",
    "\n",
    "# Classification report on the training data\n",
    "print(\"ðŸ“Œ NB Classification Report on Training Data:\")\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "# Confusion matrix on the training data\n",
    "print(\"ðŸŸ¦ NB - Confusion Matrix on Training Data:\")\n",
    "print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª NB - Validation dataset shape: (364, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "valid_path = \"/workspaces/Final-Year-Project/Cleaned Data/ValidationData.csv\"\n",
    "df_valid = pd.read_csv(valid_path)\n",
    "\n",
    "# Drop any rows with missing values in the validation set\n",
    "df_valid = df_valid.dropna()\n",
    "\n",
    "# Prepare features and target for validation data\n",
    "X_valid = df_valid.drop('Diabetes Status', axis=1)\n",
    "y_valid = df_valid['Diabetes Status']\n",
    "\n",
    "# Scale the validation data using the previously fitted scaler\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Display the shape of the validation dataset\n",
    "print(f\"ðŸ§ª NB - Validation dataset shape: {X_valid.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation data\n",
    "y_pred_valid = best_nb.predict(X_valid_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ NB - Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.66      0.72       178\n",
      "           1       0.72      0.85      0.78       186\n",
      "\n",
      "    accuracy                           0.76       364\n",
      "   macro avg       0.76      0.75      0.75       364\n",
      "weighted avg       0.76      0.76      0.75       364\n",
      "\n",
      "ðŸŸ¦ NB - Validation Confusion Matrix:\n",
      "[[117  61]\n",
      " [ 28 158]]\n"
     ]
    }
   ],
   "source": [
    "# Print classification report for validation data\n",
    "print(f\"ðŸ“Œ NB - Validation Classification Report:\")\n",
    "print(classification_report(y_valid, y_pred_valid))\n",
    "\n",
    "# Print confusion matrix for validation data\n",
    "print(\"ðŸŸ¦ NB - Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Model  Precision    Recall  F1-Score  Accuracy\n",
      "0                       AdaBoost (AB)   0.767212  0.752747  0.748389  0.752747\n",
      "1                  Neural Network(NN)   0.762241  0.755495  0.753238  0.755495\n",
      "2                   Decision Tree(DT)   0.748498  0.741758  0.739233  0.741758\n",
      "3   K Nearest Nearest Neighbours(KNN)   0.723548  0.722527  0.721802  0.722527\n",
      "4                  Neural Network(NN)   0.775001  0.769231  0.767458  0.769231\n",
      "5                   Decision Tree(DT)   0.748498  0.741758  0.739233  0.741758\n",
      "6   K Nearest Nearest Neighbours(KNN)   0.723548  0.722527  0.721802  0.722527\n",
      "7            Logistic Regression (LR)   0.759684  0.755495  0.753959  0.755495\n",
      "8            Logistic Regression (LR)   0.759684  0.755495  0.753959  0.755495\n",
      "9            Logistic Regression (LR)   0.759684  0.755495  0.753959  0.755495\n",
      "10                  Random Forest(RF)   0.744664  0.741758  0.740463  0.741758\n",
      "11                  Random Forest(RF)   0.748448  0.744505  0.742901  0.744505\n",
      "12        Support Vecotr Machine(SVM)   0.762241  0.755495  0.753238  0.755495\n",
      "13        Support Vecotr Machine(SVM)   0.765468  0.755495  0.752374  0.755495\n",
      "14        Support Vecotr Machine(SVM)   0.773500  0.763736  0.760870  0.763736\n",
      "15                 Neural Network(NN)   0.775001  0.769231  0.767458  0.769231\n",
      "16                    Naive Bayes(LR)   0.763240  0.755495  0.752966  0.755495\n",
      "17                    Naive Bayes(LR)   0.763240  0.755495  0.752966  0.755495\n",
      "18                    Naive Bayes(LR)   0.763240  0.755495  0.752966  0.755495\n",
      "19                    Naive Bayes(LR)   0.763240  0.755495  0.752966  0.755495\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you already have y_valid (true labels) and y_pred_valid (predicted labels)\n",
    "# Example classification report for AdaBoost (AB)\n",
    "report = classification_report(y_valid, y_pred_valid, output_dict=True)\n",
    "\n",
    "# Extract weighted scores\n",
    "weighted_precision = report['weighted avg']['precision']\n",
    "weighted_recall = report['weighted avg']['recall']\n",
    "weighted_f1 = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a dictionary with the scores\n",
    "ab_results = {\n",
    "    'Model': 'Naive Bayes(LR)',\n",
    "    'Precision': weighted_precision,\n",
    "    'Recall': weighted_recall,\n",
    "    'F1-Score': weighted_f1,\n",
    "    'Accuracy': accuracy\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "ab_df = pd.DataFrame([ab_results])\n",
    "\n",
    "# Create a DataFrame to store results (if not already created)\n",
    "results_df = pd.read_csv(\"/workspaces/Final-Year-Project/Results/model_results.csv\")\n",
    "\n",
    "# Concatenate the new row to the DataFrame\n",
    "results_df = pd.concat([results_df, ab_df], ignore_index=True)\n",
    "\n",
    "# Print the DataFrame to confirm it's added\n",
    "print(results_df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv(\"/workspaces/Final-Year-Project/Results/model_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ† Best Model Validation F1-Score: 0.7523525589573061\n"
     ]
    }
   ],
   "source": [
    "# Recalculate Macro average F1-score on validation data\n",
    "validation_f1_macro = f1_score(y_valid, y_pred_valid, average='macro')\n",
    "print(f\"ðŸ† Best Model Validation F1-Score: {validation_f1_macro}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'groupby'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Group means by diabetes status\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m means_by_class \u001b[38;5;241m=\u001b[39m \u001b[43mX_scaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiabetes Status\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Compute mean differences (positive class - negative class)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m mean_diff \u001b[38;5;241m=\u001b[39m means_by_class\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m means_by_class\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'groupby'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group means by diabetes status\n",
    "means_by_class = X_scaled.groupby('Diabetes Status').mean()\n",
    "\n",
    "# Compute mean differences (positive class - negative class)\n",
    "mean_diff = means_by_class.loc[1] - means_by_class.loc[0]\n",
    "mean_diff = mean_diff.sort_values(ascending=False)\n",
    "\n",
    "# Plot inline\n",
    "plt.figure(figsize=(10, 6))\n",
    "mean_diff.plot(kind='bar')\n",
    "plt.title(\"Feature Influence in Naive Bayes: Class Mean Differences (Standardized)\")\n",
    "plt.ylabel(\"Mean Difference (Diabetic - Non-Diabetic)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
