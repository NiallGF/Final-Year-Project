{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Test and Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AB - Original dataset shape: (3268, 14)\n",
      "üßπ AB - After dropping NaNs: (3268, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "data_path = '/workspaces/Final-Year-Project/Cleaned Data/TrainTestData.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the original shape of the dataset\n",
    "print(f\"üîç AB - Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the shape after dropping NaNs\n",
    "print(f\"üßπ AB - After dropping NaNs: {df.shape}\")\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop('Diabetes Status', axis=1)\n",
    "y = df['Diabetes Status']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä AB - Final dataset shape after scaling: (3268, 13)\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Display the shape after scaling\n",
    "print(f\"üìä AB - Final dataset shape after scaling: {X_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier setup\n",
    "ab = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],  # Number of base models (trees) in the ensemble\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Impact of each base model\n",
    "}\n",
    "\n",
    "# KFold cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# GridSearchCV setup with F1-score as the scoring parameter\n",
    "grid_search = GridSearchCV(ab, param_grid, scoring='f1', cv=kf, n_jobs=-1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AB - Best Parameters: {'learning_rate': 0.2, 'n_estimators': 200}\n",
      "‚úÖ AB - K-Fold Mean F1-Score with Best Parameters: 0.7674\n"
     ]
    }
   ],
   "source": [
    "# Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Best model from GridSearchCV\n",
    "best_ab = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(f\"‚úÖ AB - Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"‚úÖ AB - K-Fold Mean F1-Score with Best Parameters: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå AB Classification Report on Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.64      0.72      1638\n",
      "           1       0.70      0.86      0.77      1630\n",
      "\n",
      "    accuracy                           0.75      3268\n",
      "   macro avg       0.76      0.75      0.74      3268\n",
      "weighted avg       0.76      0.75      0.74      3268\n",
      "\n",
      "üü¶ AB - Confusion Matrix on Training Data:\n",
      "[[1041  597]\n",
      " [ 227 1403]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training data\n",
    "y_pred = best_ab.predict(X_scaled)\n",
    "\n",
    "# Classification report on the training data\n",
    "print(\"üìå AB Classification Report on Training Data:\")\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "# Confusion matrix on the training data\n",
    "print(\"üü¶ AB - Confusion Matrix on Training Data:\")\n",
    "print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ AB - Validation dataset shape: (364, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "valid_path = \"/workspaces/Final-Year-Project/Cleaned Data/ValidationData.csv\"\n",
    "df_valid = pd.read_csv(valid_path)\n",
    "\n",
    "# Drop any rows with missing values in the validation set\n",
    "df_valid = df_valid.dropna()\n",
    "\n",
    "# Prepare features and target for validation data\n",
    "X_valid = df_valid.drop('Diabetes Status', axis=1)\n",
    "y_valid = df_valid['Diabetes Status']\n",
    "\n",
    "# Scale the validation data using the previously fitted scaler\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Display the shape of the validation dataset\n",
    "print(f\"üß™ AB - Validation dataset shape: {X_valid.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation data\n",
    "y_pred_valid = best_ab.predict(X_valid_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå AB - Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71       178\n",
      "           1       0.71      0.88      0.78       186\n",
      "\n",
      "    accuracy                           0.75       364\n",
      "   macro avg       0.77      0.75      0.75       364\n",
      "weighted avg       0.77      0.75      0.75       364\n",
      "\n",
      "üü¶ AB - Validation Confusion Matrix:\n",
      "[[111  67]\n",
      " [ 23 163]]\n"
     ]
    }
   ],
   "source": [
    "# Print classification report for validation data\n",
    "print(f\"üìå AB - Validation Classification Report:\")\n",
    "print(classification_report(y_valid, y_pred_valid))\n",
    "\n",
    "# Print confusion matrix for validation data\n",
    "print(\"üü¶ AB - Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Precision    Recall  F1-Score  Accuracy\n",
      "0  AdaBoost (AB)   0.767212  0.752747  0.748389  0.752747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8804/645382371.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, ab_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you already have y_valid (true labels) and y_pred_valid (predicted labels)\n",
    "# Example classification report for AdaBoost (AB)\n",
    "report = classification_report(y_valid, y_pred_valid, output_dict=True)\n",
    "\n",
    "# Extract weighted scores\n",
    "weighted_precision = report['weighted avg']['precision']\n",
    "weighted_recall = report['weighted avg']['recall']\n",
    "weighted_f1 = report['weighted avg']['f1-score']\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "# Create a dictionary with the scores\n",
    "ab_results = {\n",
    "    'Model': 'AdaBoost (AB)',\n",
    "    'Precision': weighted_precision,\n",
    "    'Recall': weighted_recall,\n",
    "    'F1-Score': weighted_f1,\n",
    "    'Accuracy': accuracy\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "ab_df = pd.DataFrame([ab_results])\n",
    "\n",
    "# Create a DataFrame to store results (if not already created)\n",
    "results_df = pd.DataFrame(columns=['Model', 'Precision', 'Recall', 'F1-Score', 'Accuracy'])\n",
    "\n",
    "# Concatenate the new row to the DataFrame\n",
    "results_df = pd.concat([results_df, ab_df], ignore_index=True)\n",
    "\n",
    "# Print the DataFrame to confirm it's added\n",
    "print(results_df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv(\"/workspaces/Final-Year-Project/Results/model_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Best Model Validation F1-Score: 0.7475961538461539\n"
     ]
    }
   ],
   "source": [
    "# Recalculate Macro average F1-score on validation data\n",
    "validation_f1_macro = f1_score(y_valid, y_pred_valid, average='macro')\n",
    "print(f\"üèÜ Best Model Validation F1-Score: {validation_f1_macro}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Precision (Train)  Recall (Train)  F1-Score (Train)  \\\n",
      "0  AdaBoost           0.761385        0.747858          0.744657   \n",
      "1  AdaBoost                NaN             NaN               NaN   \n",
      "\n",
      "   Accuracy (Train)  Precision (Validation)  Recall (Validation)  \\\n",
      "0          0.747858                     NaN                  NaN   \n",
      "1               NaN                0.767212             0.752747   \n",
      "\n",
      "   F1-Score (Validation)  Accuracy (Validation)  \n",
      "0                    NaN                    NaN  \n",
      "1               0.748389               0.752747  \n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have the correct training data variables: X_train, y_train, and y_train_pred\n",
    "# Train results\n",
    "train_report = classification_report(y, y_pred, output_dict=True)  # Use the correct training predictions\n",
    "train_results = {\n",
    "    'Model': 'AdaBoost',\n",
    "    'Precision (Train)': train_report['weighted avg']['precision'],\n",
    "    'Recall (Train)': train_report['weighted avg']['recall'],\n",
    "    'F1-Score (Train)': train_report['weighted avg']['f1-score'],\n",
    "    'Accuracy (Train)': train_report['accuracy']\n",
    "}\n",
    "\n",
    "# Validation results\n",
    "y_valid_pred = best_ab.predict(X_valid_scaled)  # Assuming you've already processed X_valid_scaled\n",
    "valid_report = classification_report(y_valid, y_valid_pred, output_dict=True)\n",
    "valid_results = {\n",
    "    'Model': 'AdaBoost',\n",
    "    'Precision (Validation)': valid_report['weighted avg']['precision'],\n",
    "    'Recall (Validation)': valid_report['weighted avg']['recall'],\n",
    "    'F1-Score (Validation)': valid_report['weighted avg']['f1-score'],\n",
    "    'Accuracy (Validation)': valid_report['accuracy']\n",
    "}\n",
    "\n",
    "# Convert both to DataFrame and concatenate\n",
    "train_df = pd.DataFrame([train_results])\n",
    "valid_df = pd.DataFrame([valid_results])\n",
    "\n",
    "# Combine the results into one DataFrame\n",
    "results_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save the results to CSV\n",
    "results_df.to_csv(\"ada_boost_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
